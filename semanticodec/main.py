from configparser import NoSectionError
import torch
import torch.nn as nn
import os
import torchaudio
import math

from semanticodec.modules.encoder.encoder import AudioMAEConditionQuantResEncoder
from semanticodec.modules.decoder.latent_diffusion.models.ddpm import (
    extract_encoder_state_dict,
    overlap_add_waveform,
)
from semanticodec.config import get_config
from semanticodec.modules.decoder.latent_diffusion.util import instantiate_from_config
from semanticodec.utils import extract_kaldi_fbank_feature
from huggingface_hub import hf_hub_download

# Constants
SAMPLE_RATE = 16000
SEGMENT_DURATION = 10.24
MEL_TARGET_LENGTH = 1024
AUDIOMAE_PATCH_DURATION = 0.16
SEGMENT_OVERLAP_RATIO = 0.0625


class SemantiCodec(nn.Module):
    def __init__(
        self,
        token_rate,
        semantic_vocab_size,
        ddim_sample_step=50,
        cfg_scale=2.0,
        checkpoint_path = None,
        cache_path="pretrained",
        local_model_dir="/path/to/your/models",
    ):
        super().__init__()
        self.token_rate = token_rate
        self.stack_factor_K = 100 / self.token_rate
        # stack_factor_K: å®ƒçš„å«ä¹‰ä¸ºaudioMAEä¸­ patch åˆ° tokens çš„å‹ç¼©ç‡ï¼Œå…¶å€¼ä¸º  100 / self.token_rate
        # token_rateå°±æ˜¯æ¯ç§’éŸ³é¢‘å¤šå°‘ä¸ªtokensï¼Œè€Œpatchåˆ™æ˜¯å›ºå®šçš„æ¯ç§’50ä¸ªï¼ˆ10.24s/512ä¸ªï¼‰ï¼Œ
        # è€Œæ¯ä¸ªtokensåŒ…å«ä¸¤ç§ç»“æœï¼šä¸€æ˜¯audioMAEï¼Œå¦ä¸€æ˜¯æ®‹å·®çŸ¢é‡ï¼Œå› æ­¤stack_factor_K = ï¼ˆ50 / self.token_rateï¼‰ * 2
        # ä¹˜ 2 æ˜¯å› ä¸ºæ¯ä¸ªtokensåŒ…å«ä¸¤ç§ç»“æœã€‚å› æ­¤å‹ç¼©ç‡ * 2

        self.ddim_sample_step = ddim_sample_step
        self.cfg_scale = cfg_scale

        if torch.cuda.is_available():
            self.device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            self.device = torch.device("mps")
        else:
            self.device = torch.device("cpu")


        # Initialize encoder and decoder
        config, checkpoint_path, feature_dim, lstm_layers, semanticodebook = get_config(
            token_rate, semantic_vocab_size, checkpoint_path
        )
        # checkpoint_path = semanticodec_tokenrate_50
        encoder_checkpoint_path = os.path.join(checkpoint_path, "encoder.ckpt")
        if not os.path.exists(encoder_checkpoint_path):
            if not os.path.exists(cache_path):
                os.makedirs(cache_path)
                print(f"checkpoint cache dir '{cache_path}' was created.")
            encoder_checkpoint_path = hf_hub_download(repo_id="haoheliu/SemantiCodec",filename=checkpoint_path+"/encoder.ckpt",cache_dir=cache_path)
        decoder_checkpoint_path = os.path.join(checkpoint_path, "decoder.ckpt")
        if not os.path.exists(decoder_checkpoint_path):
            decoder_checkpoint_path = hf_hub_download(repo_id="haoheliu/SemantiCodec",filename=checkpoint_path+"/decoder.ckpt",cache_dir=cache_path)

        if not os.path.exists(semanticodebook):
            semanticodebook = "/".join(semanticodebook.split("/")[-3:])
            semanticodebook = hf_hub_download(repo_id="haoheliu/SemantiCodec",filename=semanticodebook,cache_dir=cache_path)

        # Initialize encoder
        print("ğŸš€ Loading SemantiCodec encoder")
        state_dict = torch.load(encoder_checkpoint_path, map_location="cpu")
        self.encoder = AudioMAEConditionQuantResEncoder(
            feature_dimension=feature_dim,
            lstm_layer=lstm_layers,
            centroid_npy_path=semanticodebook,
        )
        self.encoder.load_state_dict(state_dict)
        self.encoder = self.encoder.half()
        self.encoder = self.encoder.to(self.device)
        print("âœ… Encoder loaded")

        # Initialize decoder
        print("ğŸš€ Loading SemantiCodec decoder")
        self.decoder = instantiate_from_config(config["model"])
        checkpoint = torch.load(decoder_checkpoint_path, map_location="cpu")
        self.decoder.load_state_dict(checkpoint)
        self.decoder = self.decoder.half()
        self.decoder = self.decoder.to(self.device)
        print("âœ… Decoder loaded")

    def load_audio(self, filepath):
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"{filepath} does not exist")

        assert isinstance(filepath, str)
        waveform, sr = torchaudio.load(filepath)
        # resample to 16000
        if sr != SAMPLE_RATE:
            waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)
            sr = SAMPLE_RATE
        # if stereo to mono
        if waveform.shape[0] > 1:
            waveform = waveform[0:1]
        # Calculate the original duration
        original_duration = waveform.shape[1] / sr
        # This is to pad the audio to the multiplication of 0.16 seconds so that the original audio can be reconstructed
        # ä¸ºä»€ä¹ˆè¦æ±‚éŸ³é¢‘æ—¶é•¿ä¸º0.16Sæ•´æ•°å€ï¼Ÿ  å› ä¸ºåœ¨patchæå–è¿‡ç¨‹ä¸­ä½¿ç”¨åˆ°çš„å·ç§¯æ ¸å’Œæ­¥é•¿å‡ä¸º(16, 16)ï¼Œ
        # å…¶æ„ŸçŸ¥é‡ä¸º16å¸§melï¼Œæ¯å¸§melä¸º10msï¼Œå› æ­¤éœ€è¦éŸ³é¢‘æ—¶é•¿å¿…é¡»ä¸º0.16sçš„æ•´æ•°å€ï¼Œå¦åˆ™ä¼šå‡ºç°ä¸å®Œæ•´çš„ patchã€‚
        original_duration = original_duration + (
            AUDIOMAE_PATCH_DURATION - original_duration % AUDIOMAE_PATCH_DURATION
        )
        # Calculate the token length in theory
        # stack_factor_K: å®ƒçš„å«ä¹‰ä¸ºaudioMAEä¸­ patch åˆ° tokens çš„å‹ç¼©ç‡ï¼Œå…¶å€¼ä¸º  100 / self.token_rate
        # token_rateå°±æ˜¯æ¯ç§’éŸ³é¢‘å¤šå°‘ä¸ªtokensï¼Œè€Œpatchåˆ™æ˜¯å›ºå®šçš„æ¯ç§’50ä¸ªï¼ˆ10.24s/512ä¸ªï¼‰ï¼Œ
        # è€Œæ¯ä¸ªtokensåŒ…å«ä¸¤ç§ç»“æœï¼šä¸€æ˜¯audioMAEï¼Œå¦ä¸€æ˜¯æ®‹å·®çŸ¢é‡ï¼Œå› æ­¤stack_factor_K = ï¼ˆ50 / self.token_rateï¼‰ * 2
        # ä¹˜ 2 æ˜¯å› ä¸ºæ¯ä¸ªtokensåŒ…å«ä¸¤ç§ç»“æœã€‚å› æ­¤å‹ç¼©ç‡ * 2
        target_token_len = (
            8 * original_duration / AUDIOMAE_PATCH_DURATION / self.stack_factor_K
        )
        segment_sample_length = int(SAMPLE_RATE * SEGMENT_DURATION)
        # Pad audio to the multiplication of 10.24 seconds for easier segmentations
        # ä¸ºä»€ä¹ˆè¦æ‰©å±•åˆ°10.24sçš„æ•´æ•°å€ï¼Ÿ
        # audioMAEçš„è¾“å…¥ä¸º(bs,1024,128)çš„melï¼Œæ¯æ¬¡å¿…é¡»è¦è¾“å…¥1024å¸§melï¼Œä¹Ÿå°±æ˜¯10.24sçš„éŸ³é¢‘ã€‚
        if waveform.shape[1] % segment_sample_length < segment_sample_length:
            waveform = torch.cat(
                [
                    waveform,
                    torch.zeros(
                        1,
                        int(
                            segment_sample_length
                            - waveform.shape[1] % segment_sample_length
                        ),
                    ),
                ],
                dim=1,
            )

        mel_target_length = MEL_TARGET_LENGTH * int(
            waveform.shape[1] / segment_sample_length
        )
        # Calculate the mel spectrogram
        mel = extract_kaldi_fbank_feature(
            waveform, sr, target_length=mel_target_length
        )["ta_kaldi_fbank"].unsqueeze(0)
        mel = mel.squeeze(1)
        assert mel.shape[-1] == 128 and mel.shape[-2] % 1024 == 0
        return mel, target_token_len

    def encode(self, filepath):
        mel, target_token_len = self.load_audio(filepath)
        mel = mel.half()
        tokens = self.encoder(mel.to(self.device))
        tokens = tokens[:, : math.ceil(target_token_len), :]
        # tokens ä¸ºaudioMAEçš„é‡åŒ–å’Œæ®‹å·®çŸ¢é‡é‡åŒ–çš„æ‹¼æ¥ï¼Œå…¶ç»´åº¦ä¸º(1,tokenï¼Œ2),2å³ä¸ºä¸¤ç§é‡åŒ–ç»“æœ
        return tokens

    def decode(self, tokens):
        # tokens ä¸ºaudioMAEçš„é‡åŒ–å’Œæ®‹å·®çŸ¢é‡é‡åŒ–çš„æ‹¼æ¥ï¼Œå…¶ç»´åº¦ä¸º(1,tokenï¼Œ2),2å³ä¸ºä¸¤ç§é‡åŒ–ç»“æœ
        # é¦–å…ˆï¼Œ å°†æ‰€æœ‰tokens é‡æ–°åˆ†å‰²ä¸ºå¯¹åº”patchæ ¼å¼çš„æ•°æ®æ®µï¼Œ
        # ä¾‹å¦‚å¦‚æœstack_factor_Kä¸º4ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ512ä¸ªpatch å¯¹åº”çš„ ä¸º128ä¸ª token  ï¼ˆæ³¨æ„æ˜¯ ï¼ˆ1ï¼Œ128ï¼Œ2ï¼‰ï¼‰
        # *** 512ä¸ªpatchä¸º10.24sï¼Œå¦‚æœstack_factor_Kä¸º4ï¼Œå³token_reteå‚æ•°ä¸º25ï¼Œä¹Ÿå°±æ˜¯æ¯ç§’25ä¸ªtoken, 10.24så°±æœ‰256ä¸ªtoken,å› æ­¤audioMAEå’Œæ®‹å·®å„å 128tokenï¼Œ
        # å› æ­¤å¦‚æœstack_factor_Kä¸º4ï¼Œé‚£ä¹ˆtokensä¸º(1,tokenï¼Œ2),windowed_token_liståˆ™åŒ…å«äº†è¯¸å¤š(1,128ï¼Œ2)
        windowed_token_list = self.encoder.long_token_split_window(
            tokens,
            window_length=int(512 / self.stack_factor_K),  #stack_factor_Kä¸ºaudioMAEä¸­ patch åˆ° tokens çš„å‹ç¼©ç‡
            overlap=SEGMENT_OVERLAP_RATIO,
        )   #å°† tokensï¼ˆ[1, token, 2]ï¼‰åˆ†å‰²ä¸ºå¤šä¸ªçª—å£ï¼ˆæ¯ä¸ª [1, 128, 2]ï¼‰ã€‚
        windowed_waveform = []
        for _, windowed_token in enumerate(windowed_token_list):
            # éå†è§£ç 
            # é¦–å…ˆå¤ç°æ½œåœ¨ç‰¹å¾latent ä¾‹å¦‚ [1, token, 6144]
            latent = self.encoder.token_to_quantized_feature(windowed_token)
            latent = torch.cat(
                [
                    latent,
                    torch.ones(
                        latent.shape[0],
                        int(512 / self.stack_factor_K) - latent.shape[1],
                        latent.shape[2],
                    ).to(latent.device)    #å¦‚æœtokenä¸è¶³10.24sï¼Œ è¿™é‡Œä»ç„¶éœ€è¦å¡«å……ä¸€ä¸‹
                    * -1,
                ],
                dim=1,
            )
            latent = latent.half()
            # å°† latent è§£ç ä¸ºæ³¢å½¢
            waveform = self.decoder.generate_sample(
                latent,
                ddim_steps=self.ddim_sample_step,
                unconditional_guidance_scale=self.cfg_scale,
            )
            windowed_waveform.append(waveform)
        output = overlap_add_waveform(
            windowed_waveform, overlap_duration=SEGMENT_DURATION * SEGMENT_OVERLAP_RATIO
        )  #æ‹¼æ¥å¤šä¸ªçª—å£æ³¢å½¢
        # Each patch step equal 16 mel time frames, which have 0.01 second
        trim_duration = (tokens.shape[1] / 8) * 16 * 0.01 * self.stack_factor_K
        return output[..., : int(trim_duration * SAMPLE_RATE)]

    def forward(self, filepath):
        with torch.cuda.amp.autocast():
            tokens = self.encode(filepath)
            #tokens = tokens.half()
            waveform = self.decode(tokens)
        return waveform


from thop import profile, clever_format
from torch.autograd import Variable
import torch

if __name__ == '__main__':
    # Initialize the SemantiCodec model
    net = SemantiCodec(
        token_rate=25,  # Example token rate
        semantic_vocab_size=4096,  # Example vocab size
        ddim_sample_step=50,
        cfg_scale=2.0,
        checkpoint_path = 'F:\Project\VoiceCodec\SemantiCodec\pretrained\SemantiCodec',  # Adjust as needed
        cache_path="pretrained",
        local_model_dir="/path/to/your/models"
    )

    # Prepare a sample input (file path to an audio file)
    sample_audio_path = r"F:\Project\VoiceCodec\SemantiCodec\testData\float16_output\16k-hybr.wav"  # Replace with a valid audio file path

    mel, target_token_len = net.load_audio(sample_audio_path)
    mel = mel.to(net.device).half()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net = net.to(device)
    net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    data_input = torch.randn(1, 10240, 128).to(device).half()
    # Profiling ç¼–ç å™¨
    print("æ­£åœ¨åˆ†æç¼–ç å™¨...")
    flops_enc, params_enc = profile(net.encoder, inputs=(mel,))
    flops_enc, params_enc = clever_format([flops_enc, params_enc], "%.3f")
    print(f"ç¼–ç å™¨å‚æ•°é‡: {params_enc}")
    print(f"ç¼–ç å™¨ FLOPs: {flops_enc}")

    # è·å– tokens
    tokens = net.encode(sample_audio_path)

    # Profiling è§£ç å™¨
    print("æ­£åœ¨åˆ†æè§£ç å™¨...")
    flops_dec, params_dec = profile(net.decoder, inputs=(tokens,))
    flops_dec, params_dec = clever_format([flops_dec, params_dec], "%.3f")
    print(f"è§£ç å™¨å‚æ•°é‡: {params_dec}")
    print(f"è§£ç å™¨ FLOPs: {flops_dec}")

    # æ€»è®¡
    total_params = float(params_enc[:-1]) + float(params_dec[:-1])  # ç§»é™¤å•ä½ï¼ˆM/Gï¼‰
    total_flops = float(flops_enc[:-1]) + float(flops_dec[:-1])
    total_params, total_flops = clever_format([total_params * 1e6, total_flops * 1e9], "%.3f")
    print(f"æ€»å‚æ•°é‡: {total_params}")
    print(f"æ€» FLOPs: {total_flops}")